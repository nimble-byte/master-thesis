\section{Methodology} \label{sec:methodology}

To investigate the effect of explainable generative \ac{AI} on users' problem-solving process and their metacognitions a grounded-theory approach based on \cite{Gioia2013} was chosen. The following sections will present the design and execution of the study, as well as the procedure for data analysis inspired by \cite{Jussupow2021}.

\subsection{Research Design} \label{ssec:research_design}

% What data was collected
The 13 participants were tasked with solving maths problems with the help of a chatbot, that was tuned to assist the users without solving the problems for them. Qualitative data about the problem-solving approach was collected as think-aloud protocols, while additional quantitative data about the perception of the chatbot was recorded using questionnaires. To investigate the effects of explanations, participants were split into two separate groups, that provided different chatbots: Group (A) was using a chatbot, that provided an explanation for its outputs, group (B) did not receive an explanation. Both groups were served tasks from the same pool and answered the same questionnaires after completing the tasks.

\subsubsection{Tasks} \label{sssec:tasks}

% TODO: consider citation idea to use maths problems?
The problems given to the participants needed to be of sufficient complexity to require assistance from the chatbot, while still being solvable by a wide range of potential participants. Ideally the problems should offer multiple solution strategies, to allow for a variety of approaches. To save time during the study design, the problems were selected form an existing dataset. While the MathVista dataset \parencite{MathVista2024} was designed for evaluating \ac{AI} systems, it can be reused for this study, since the problems are posed in natural language and often require combining visual and textual information to solve them. The selection was performed to ensure that the problems were solvable by \acs{STEM}-students, taking into account the clarity and difficulty of the problems. The final selection consisted of 15 problems from the domain of geometry.

Participants were given sets of three random problems selected, with the constraint that only “medium” and “difficult” problems were included during the study. The problems were presented digitally in a custom web application. The participant were shown the task image, text, and answer options at once. The tool recorded participants responses and time taken to solve the tasks. In case a participant was unable to solve a problem, it was replaced by a new problem of the same difficulty.

\subsubsection{Chatbot Design} \label{sssec:chatbot_design}

The design of the chatbot was driven by several requirements and constraints, derived from the research questions and practical considerations. The main consideration was to ensure that the chatbot would assist the participants, without solving the problems for them. Additionally, the chatbot should support image inputs, as the problems included visual information. Lastly, that chatbot should be able to provide a \ac{CoT} explanations, to allow for comparison between users with and without explanations.

In addition to the research-related requirements described above, practical considerations also influenced the design and implementation of the chatbot. The chatbot needed to be either runnable on a laptop or accessible via a free API, to keep the cost of the study low. Additionally, the implementation of the chatbot should be build using existing software as much as possible. Due to a lack of free API options that sufficed the research requirements, the chatbot was implemented locally using LM Studio \parencite{ElementLabs2025}, which allowed to experiment with different models and configurations before the study. It also allowed to easily experiment with additional system prompts, that could be used to tailor the responses of the model to the specific use case.

The available models were filtered automatically to only include models that were runnable on a M4 MacBook Pro (12cpu, 24 GB RAM) according to the program. After curating the list of available models and eliminating identical models of different sizes, the following options remained:

\begin{itemize}
    \item Qwen 3 (30B A3B) lacks vision capabilities, but has the unique ability to turn \ac{CoT} explanations on and off using the \texttt{/nothink} command in prompts \parencite{Qwen2025}.

    \item Gemma 3 (27B) offers vision capabilities and is the largest model, that can reliably be used on the device available. It lacks explicit \ac{CoT} explanation capabilities \parencite{GemmaTeam2025}.

    \item Mistral Small 3.2 is the newest of the three architectures and has vision capabilities. Like Gemma 3 it lacks \ac{CoT} explanations \parencite{Mistral2025}.
\end{itemize}

Initial experiments yielded, that vision capabilities would likely be of high importance. This meant Qwen 3 would need to be equipped with visual capabilities. Additionally, Qwen struggled with following the system prompts, forbidding to reveal the solution to the user, especially when \ac{CoT} explanations where enabled. It was therefore removed from the shortlist.

The remaining two models were tested using identical 2 identical system prompts. Both prompts instructed the model to act as helpful assistant, that would not solve the maths problems for the user, but instead provide guidance and hints. The difference between the prompts was that one prompt instructed the model to provide a \ac{CoT} for each response wrapped in a \texttt{<think>} tag. Both versions of the prompt can be found in the appendix. The results of the tests showed, that Mistral Small 3.2 was more reliable in following the instructions, while Gemma 3 often revealed the solution to the user, despite the system prompt. Mistral Small 3.2 was therefore chosen as the model for the chatbot.

\subsubsection{Experiment Procedure} \label{sssec:experiment_procedure}

% TODO: Add citiations
At the beginning of the experiment, the participants were given a short introduction to the study and the tools they would be using. After explaining the concept of think-aloud protocols, they were given a practice task and feedback afterwards. During the study the participants were asked to solve three randomly selected tasks after one another. They were given the chatbot (opened in a separate window from the tasks), a scientific calculator, and pen and paper for making notes. They were allowed to freely use all assistance they were offered at their discretion and afforded as much time as they wanted to solve tasks. Once users completed the tasks, they were given a set of questionnaires to fill out.

During the experiment qualitative and quantitative data was collected in several ways. Qualitative data was primarily collected by conducting experiments using the think-aloud method \parencite{VanSomeren1994}. Additionally, the conversation of users with a chatbot were recorded, to be combined with the transcripts of the think aloud protocols. A cover story was provided to the users for the experiment, telling them the goal of the study was the evaluation and comparison of the different chatbot versions \parencite{VanSomeren1994, Jussupow2021}. Lastly some participants proactively provided additional feedback after completing the experiment, which was recorded with notes by the researchers. The data was transcribed using OpenAI Whisper \parencite{Radford2022} using the “medium” sized model and specifying German as language. Afterwards the transcripts were validated against the original audio recordings and combined with the applicable chat histories to a single transcript per task.

Quantitative data was gathered using a number of post-experiment questionnaires to gather data about user's acceptance, self-efficacy, and mental load during the tasks. The used questionnaires used were \ac{TAM} \parencite{Davis1989}, \ac{CSE} \parencite{Compeau1995}, and NASA-\ac{TLX} \parencite{Hart1988}. As the study was performed in German translations for the questions and responses were required. Research showed, the NASA-\ac{TLX} questionnaire \parencite{Flaegel2019} and \ac{TAM} \parencite{Jockisch2010} were available translated and validated, but both versions were adopted for specific studies, which made them insufficient for this study. Therefore, the questionnaires were manually translated. The translations can be found in the Appendix.

\subsubsection{Participants} \label{sssec:participants}

Participants were selected from university students, colleagues, friends, and acquaintances. The primary requirement for participants was a background in \ac{STEM}, either through their studies or profession. This was to ensure that participants had sufficient prior maths knowledge to solve the problems. The participants were split into two groups pseudo randomly in order of their registration, alternating between the two groups. The final sample consisted of 13 participants, with 7 in group (B) without explanations and 6 in group (E) with explanations.

\subsection{Data Analysis} \label{ssec:data_analysis}

% - primary goal was to preserve data-to-theory connection (Gioia2021)
% - performed qualitative and quantitative evaluation
% - qualitative evaluation done primarily following coding methods from Saldana2015
%   - descriptive coding: get initial overview over data, get a feeling for coding process
%   - process coding: extract relevant process steps/elements from the transcripts
%   - axial coding: group/organise the first cycle codes
%   - theoretical coding: develop theory/process model from the previously grouped codes
% - quantitative data was evaluated by performing basic statistical analysis
%   - caveat: 13 participants are too few with to perform significant statistical analysis
%   - validate data againast some known facts to sanity check
%   - determine differences between model with explanations and without
%     - increased acceptance (Li2021)
%     - reduced mental load
