\section{Introduction} \label{sec:introduction}

The advent of generative \ac{AI} has sparked unprecedented public interest in \ac{AI} with a wide range of applications, from text generation \parencite{OpenAI2022} to image synthesis \parencite{Rombach2021}. This public interest has driven significant advancements in the field of generative \ac{AI} and widespread adoption across various industries. In particular \acp{LLM} have gained attention due to their versatility and consequently have been widely adopted in the form of chatbots, such as ChatGPT \parencite{OpenAI2025} or Claude \parencite{AnthropicInc2025}. Through advances like reasoning \parencite{OpenAI2024a} and tool use \parencite{AnthropicInc2024} the technical capabilities of \acp{LLM} have outpaced the research on explainability and human-\ac{AI} interaction.

Interpretability is widely recognized as essential for building trust and fostering acceptance of \ac{AI} systems among users \parencite{Arrieta2020,Shin2021}. \ac{XAI} research has provided various methods to explain the outputs of discriminative \ac{AI} systems with LIME \parencite{Ribeiro2016} and SHAP \parencite{Lundberg2017} being two of the most prominent examples. However, these models are not directly applicable to generative \ac{AI}, due to the different nature of their outputs.

With growing adoption of \ac{AI} systems — also in critical domains like healthcare and infrastructure — the need for responsible and effective utilization of \ac{AI} outputs becomes paramount. Some recent works have investigated the cognitive and metacognitive aspects of human-\ac{AI} interaction as well as the perception of \ac{AI} systems, showing the importance of correct handling of \ac{AI} outputs for effective collaboration \parencite{Jussupow2021,Kazemitabaar2024}. However, the effects of explanations on the interaction with generative \ac{AI} systems remain largely unexplored, creating a research gap.

These developments give rise to three key problems that this thesis aims to address:

\begin{enumerate}
    \item Existing explainability techniques are primarily designed for discriminative models and do not adequately address the unique challenges posed by generative models. New methods are needed to provide meaningful explanations for the outputs of generative \ac{AI} systems, that can be understood by lay users.
    \item While some studies have explored the cognition and metacognition of \ac{AI}-assisted decision-making, there is no research on the effects of explanations or cognition and metacognition in the context of generative \ac{AI}.
    \item The studies performed so far have mostly focused on qualitative analysis creating a need for quantitative research to validate findings and provide generalizable insights into human-\ac{AI} interaction with generative models.
\end{enumerate}

These problems are exacerbated by increasing regulation of \ac{AI} systems, such as the \ac{EU} \ac{AI} Act \parencite{EuropeanUnion2024}, which mandates transparency and accountability for almost all \ac{AI} applications. Addressing these challenges is crucial for ensuring that generative \ac{AI} systems are not only powerful but also trustworthy and user-friendly.

\subsection{Research Objectives} \label{subsec:research-objectives}

This thesis aims to contribute to the field of explainable generative \ac{AI} by contributing insights into explainability techniques and their effects on user cognition and metacognition when interacting with generative \ac{AI} systems. Two key research questions guide this work:

\begin{enumerate}
    \item How does explainable generative \ac{AI} problem-solving strategies?
    \item What metacognitive processes do users engage when using explainable generative \ac{AI} for problem-solving?
\end{enumerate}

To address these questions, this thesis will perform a short literature review of existing explainability techniques, with a focus on their applicability to generative \ac{AI} systems. Additionally, research on cognition and metacognition in human-\ac{AI} interaction will be summarized. Based on the findings, a user study will be designed to qualitatively and quantitatively assess the effects of explanations on user cognition and metacognition when interacting with generative \ac{AI} systems. Based on existing literature the following hypothesis are formulated to be tested in the user study:

\begin{enumerate}[label=(\textbf{H{\arabic*}})]
    \item Users exhibit a heuristic systematic approach to their interaction with generative \ac{AI} when solving problems collaboratively \parencite{Jussupow2021}.
    \item Providing explanations for the outputs of a model leads to increased acceptance of the system compared to non-explainable systems \parencite{Li2022}.
    \item Users perceive increased self-efficacy over non-explainable \ac{AI} when provided with explanations for the outputs of a model.
    \item Users feel reduced cognitive load and reduced stress when using explainable generative compared to non-explainable \ac{AI} systems.
\end{enumerate}


\subsection{Thesis Structure} \label{subsec:thesis-structure}

The remainder of this thesis is structure as follows: Section \ref{sec:state_of_research} provides an overview of the current state of research in explainable generative \ac{AI} and cognitive psychology. It summarizes existing explainability techniques and their applicability to generative \ac{AI} systems as well as relevant research from cognitive psychology. In Section \ref{sec:methodology}, the research methodology is outlined, including the design of the user study, the required chatbot, data collection methods, and analysis procedure. The results of the study are presented in Section \ref{sec:results}, followed by a discussion of the findings in Section \ref{sec:discussion}, including implications for future research and practical applications. Finally, Section \ref{sec:conclusion} concludes the thesis by summarizing the key contributions and suggesting directions for future work in the field of explainable generative \ac{AI}.
