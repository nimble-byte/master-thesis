\section{Introduction} \label{sec:introduction}

The advent of generative \ac{AI} has sparked unprecedented public interest in \ac{AI} with a wide range of applications, from text generation to image synthesis \parencite{OpenAI2022,Rombach2021}. This public interest has driven significant advancements in the field of generative \ac{AI} and widespread adoption across various industries. In particular \acp{LLM} have gained significant attention due to their diverse capabilities and consequently have been widely adopted in the form of chatbots, such as ChatGPT \parencite{OpenAI2025} or Claude \parencite{AnthropicInc2025}. Through advances like reasoning and tool use the technical capabilities of \acp{LLM} have outpaced the research on explainability and human-\ac{AI} interaction \parencite{OpenAI2024a, AnthropicInc2024}.

Interpretability is widely recognized as essential for building trust and fostering acceptance of \ac{AI} systems among users \parencite{Arrieta2020,Shin2021}. \ac{XAI} research has provided various methods to explain the outputs of discriminative \ac{AI} systems with LIME and SHAP being two of the most prominent examples \parencite{Ribeiro2016,Lundberg2017}. However, these methods face limitations when applied to generative models, which lack established frameworks for explanation. Additionally, increasing regulatory requirements, such as the \ac{EU} AI Act, mandate transparency and accountability for \ac{AI} systems, further emphasizing the need for reliable explainability techniques \parencite{EuropeanUnion2024}.

With growing adoption of \ac{AI} systems — also in critical domains like healthcare and infrastructure — the need for responsible and effective utilization of \ac{AI} outputs becomes paramount. Some recent works have investigated the cognitive and metacognitive aspects of human-\ac{AI} interaction as well as the perception of \ac{AI} systems, showing the importance of correct handling of \ac{AI} outputs for effective collaboration \parencite{Jussupow2021,Kazemitabaar2024}. However, the effects of explanations on the interaction with generative \ac{AI} systems remain largely unexplored, creating a research gap.

These developments give rise to three key problems that this thesis aims to address:

\begin{enumerate}
    \item Existing explainability techniques are primarily designed for discriminative models and do not adequately address the unique challenges posed by generative models. New methods are needed to provide meaningful explanations for the outputs of generative \ac{AI} systems, that can be understood by lay users.
    \item While some studies have explored the cognition and metacognition of \ac{AI}-assisted decision-making, there is no research on the effects of explanations or cognition and metacognition in the context of generative \ac{AI}.
    \item The studies performed so far have mostly focused on qualitative analysis creating a need for quantitative research to validate findings and provide generalizable insights into human-\ac{AI} interaction with generative models.
\end{enumerate}

\subsection{Research Objectives} \label{subsec:research-objectives}


\subsection{Thesis Structure} \label{subsec:thesis-structure}
