\section{Introduction} \label{sec:introduction}

The advent of generative \ac{AI} has sparked unprecedented public interest in \ac{AI} with a wide range of applications, from text generation \parencite{OpenAI2022} to image synthesis \parencite{Rombach2021}. This public interest has driven significant advancements in the field of generative \ac{AI} and widespread adoption across various industries. In particular \acp{LLM} have gained attention due to their versatility and consequently have been widely adopted in the form of chatbots, such as ChatGPT \parencite{OpenAI2025} or Claude \parencite{AnthropicInc2025}. Through advances like reasoning \parencite{OpenAI2024a} and tool use \parencite{AnthropicInc2024} the technical capabilities of \acp{LLM} have outpaced the research on explainability and human-\ac{AI} interaction.

Interpretability is widely recognized as essential for building trust and fostering acceptance of \ac{AI} systems among users \parencite{Arrieta2020,Shin2021}. \ac{XAI} research has provided various methods to explain the outputs of discriminative \ac{AI} systems with LIME \parencite{Ribeiro2016} and SHAP \parencite{Lundberg2017} being two of the most prominent examples. However, these models are not directly applicable to generative \ac{AI}, due to the different nature of their outputs.

With growing adoption of \ac{AI} systems — also in critical domains like healthcare and infrastructure — the need for responsible and effective utilization of \ac{AI} outputs becomes paramount. Some recent works have investigated the cognitive and metacognitive aspects of human-\ac{AI} interaction as well as the perception of \ac{AI} systems, showing the importance of correct handling of \ac{AI} outputs for effective collaboration \parencite{Jussupow2021,Kazemitabaar2024}. However, the effects of explanations on the interaction with generative \ac{AI} systems remain largely unexplored, creating a research gap.

These developments give rise to three key problems that this thesis aims to address:

\begin{enumerate}
    \item While some studies have explored the cognition and metacognition of \ac{AI}-assisted decision-making, there is no study investigating cognition and metacognition in the context of generative \ac{AI}. Understanding how users process and reflect on information provided by generative \ac{AI} systems is crucial for designing effective human-\ac{AI} collaboration.
    \item The studies performed so far have mostly focused on qualitative analysis creating a need for quantitative research to validate findings and provide generalizable insights into human-\ac{AI} interaction with generative models.
\end{enumerate}

These problems are exacerbated by increasing regulation of \ac{AI} systems, such as the \ac{EU} \ac{AI} Act \parencite{EuropeanUnion2024}, which mandates transparency and accountability for almost all \ac{AI} applications. Addressing these challenges is crucial for ensuring that generative \ac{AI} systems are not only powerful but also trustworthy and user-friendly.

\subsection{Research Objectives} \label{subsec:research-objectives}

This thesis aims to contribute to the field of explainable generative \ac{AI} by contributing insights into explainability techniques and their effects on user cognition and metacognition when interacting with generative \ac{AI} systems. Two key research questions guide this work:

\begin{enumerate}[label=(\textbf{RQ\arabic*}),leftmargin=4em]
    \item How does explainable generative \ac{AI} affect problem-solving strategies?
    \item What metacognitive processes do users engage in when using explainable generative \ac{AI} for problem-solving?
\end{enumerate}

To address these questions, this thesis will perform a short literature review of existing explainability techniques, with a focus on their applicability to generative \ac{AI} systems. Additionally, research on cognition and metacognition in human-\ac{AI} interaction will be summarized. Based on the findings, a user study will be designed to qualitatively and quantitatively assess the effects of explanations on user cognition and metacognition when interacting with generative \ac{AI} systems. Based on existing literature the following hypotheses are formulated to be tested in the user study:

\begin{enumerate}[label=(\textbf{H{\arabic*}}),leftmargin=4em]
    \item Users exhibit a heuristic-systematic approach to their interaction with generative \ac{AI} when solving problems collaboratively \parencite{Jussupow2021}.
    \item Providing explanations for the outputs of a model leads to increased acceptance of the system compared to non-explainable systems \parencite{Li2022}.
    \item Users perceive increased self-efficacy over non-explainable \ac{AI} when provided with explanations for the outputs of a model.
    \item Users feel reduced cognitive load and reduced stress when using explainable generative \ac{AI} compared to non-explainable \ac{AI} systems.
\end{enumerate}


\subsection{Thesis Structure} \label{subsec:thesis-structure}

The remainder of this thesis is structured as follows: Section \ref{sec:state_of_research} provides an overview of the current state of research in explainable generative \ac{AI} and cognitive psychology. It summarizes existing explainability techniques and their applicability to generative \ac{AI} systems as well as relevant research from cognitive psychology. In Section \ref{sec:methodology}, the research methodology is outlined, including the design of the user study, the required chatbot, data collection methods, and analysis procedure. The results of the study are presented in Section \ref{sec:results}, followed by a discussion of the findings in Section \ref{sec:discussion}, including implications for future research and practical applications.
