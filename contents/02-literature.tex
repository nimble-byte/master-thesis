\section{State of Research} \label{sec:state_of_research}

The following section provides an overview of the current state of research in \ac{XAI} and cognitive psychology. It defines key terminology, presents notable explainability methods, and discusses relevant psychological theories and findings.
% TODO: note strucutre

\subsection{Terminology} \label{ssec:terminology}

% use distinction of "interpretability" vs. "explainibility" in cognitive psychology as hook to define termns

While in the field of \ac{XAI} the terms \textit{interpretability}, \textit{explainability} and \textit{understandability} are often used interchangeably, in cognitive psychology they have distinct meanings. \textit{Interpretability} is a broad concept that encompasses various ways to “provide the meaning in understandable terms to a human” \parencite{Arieta2020}. \textit{Explainability} is one mode of these modes \parencite{Lipton2016}, that uses an explanation to convey that meaning. \textit{Transparency} is another mode of \textit{interpretability} that conveys meaning by making the inner workings of a model visible to a human \parencite{Arieta2020}. Lastly, \textit{understandability} is a term often used as a synonym for \textit{interpretability} in software engineering \parencite{Alonso2018}.

The most important distinction for this thesis is between generative and discriminative models. Conceptually generative and discriminative \ac{AI} both use machine learning to learn patterns from data. They are however fundamentally different in their application. Discriminative models learn the decision boundary between two or more classes and are used for classifying data points into these existing classes. Common use cases include anomaly detection \parencite{Edozie2025, Hilal2022}, image classification \parencite{Lu2007}, and sentiment analysis \parencite{Wankhade2022}. Generative models on the other hand learn the underlying distribution of a dataset and can be used to generate new data points that are similar to the training data. Generative models are frequently used for text generation \parencite{Brown2020} or image synthesis \parencite{Rombach2021}.

In the context of \ac{XAI} generative and discriminative models differ in one key aspect: the type and size of output they produce. In an information theoretical sense, discriminative models produce a small amount of information, like a class label that can be expressed in possibly a few dozen bits in extreme cases \parencite{Schneider2024}. Furthermore, the output is often previously known in the form of existing classes. Generative models on the other hand produce a large amount of information, like a paragraph of text or an image, that can be expressed in the order of megabits \parencite{Schneider2024}. Furthermore, the output is often novel and not previously known. This difference in output has implications for explainability methods and how users interact with and perceive these models. Discriminative models can be designed to be inherently interpretable by using simple models like decision trees or linear regression \parencite{Rudin2019}. Alternatively post-hoc explainability methods can be used to explain complex models like deep neural networks \parencite{Ribeiro2016, Lundberg2017}. Generative models on the other hand are often complex and not interpretable by design, which limits the applicability of inherently interpretable models.

\subsection{Explainability Methods} \label{ssec:explainability_methods}

Much research in \ac{XAI} has focused on developing explainability methods for discriminative models. These methods can be broadly categorized into two groups: transparency-based methods and post-hoc explainability methods \parencite{Arieta2020}. Transparency-based methods aim to make the inner workings of a model visible to a human. This can be achieved by using simple models like decision trees or linear regression that are inherently interpretable \parencite{Rudin2019}. Post-hoc explainability methods on the other hand aim to explain the output of a model without making its inner workings visible. This can be achieved highlighting important features in the input data (like LIME \parencite{Ribeiro2016}) or by approximating the model with a simpler interpretable model (like SHAP \parencite{Lundberg2017}). While these methods have proven to be effective in explaining models, both approaches are very technical and are difficult to understand for non-experts \parencite{Martens2025}.

Recently, new explainability approaches have been proposed that seek to remedy these shortcomings. Counterfactual explanations \parencite{Wachter2017} provide explanations by explaining why an alternative output was not chosen. For example in a loan application, a counterfactual explanation could state that if the applicant had a higher income, the loan would have been approved. Another approach are narrative explanations which use natural language to construct a short narrative around SHAP or counterfactual explanations \parencite{Martens2025}.

A third approach involves \ac{CoT} explanations \parencite{Wei2022}, where \acp{LLM} generate intermediate reasoning steps that lead to the final output. These steps can enhance users' perception of transparency by allowing them to follow the model's reasoning process. Moreover, decomposing complex problems into smaller steps has been shown to improve the effectiveness of \acp{LLM} in problem-solving \parencite{Wei2022}. Notably, this technique can be applied to existing models without retraining, as demonstrated in this study. However, it is important to recognize that these intermediate steps are generated by the model itself and may be unfaithful or incorrect \parencite{Turpin2023, Schneider2024}. In a related line of research, \textcite{Lindsey2025} demonstrated that it is possible to identify and visualize specific neurons representing concepts, and to observe activation patterns that resemble a thought process.

% % New proposed structure for the section:
% \subsection{Explainable Artificial Intelligence (XAI)} \label{ssec:xai}

% \ac{XAI}

% \cite{Arieta2020}

% \subsubsection{Discriminative vs. Generative Models} \label{sssec:disc_vs_gen}

% \subsubsection{Transparency and Post-hoc Explainability} \label{sssec:transparency_posthoc}

% \subsubsection{Established Explainability Methods} \label{sssec:established_methods}

% \cite{Ribeiro2016}

% \cite{Lundberg2017}

% \subsubsection{Novel Explainability Approaches} \label{sssec:novel_approaches}

% \cite{Martens2025}

% \subsection{Cognitive Psychology} \label{ssec:cognitive_psychology}

% \subsubsection{Dual Process Theory and Heuristics} \label{sssec:dual_process}

% \cite{Kahnemann2011}

% \subsubsection{Cognitive Offloading} \label{sssec:cognitive_offloading}

% \subsubsection{Metacognition} \label{sssec:metacognition}

% \subsubsection{Psychology of Explanations} \label{sssec:psychology_explanations}

% \cite{Miller2019}

% \subsubsection{Cognition and Metacognition of AI Decisions} \label{sssec:cognition_metacognition_ai}

% \cite{Jussupow2021}

% \cite{Shin2021}
