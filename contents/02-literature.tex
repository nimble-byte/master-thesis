\section{State of Research} \label{sec:state_of_research}

The following section provides an overview of the current state of research in \ac{XAI} and cognitive psychology. It defines key terminology, presents notable explainability methods, and discusses relevant psychological theories and findings.
% TODO: note strucutre

\subsection{Terminology} \label{ssec:terminology}

% use distinction of "interpretability" vs. "explainibility" in cognitive psychology as hook to define termns

While in the field of \ac{XAI} the terms \textit{interpretability}, \textit{explainability} and \textit{understandability} are often used interchangeably, in cognitive psychology they have distinct meanings. \textit{Interpretability} is a broad concept that encompasses various ways to “provide the meaning in understandable terms to a human” \parencite{Arieta2020}. \textit{Explainability} is one mode of these modes \parencite{Lipton2016}, that uses an explanation to convey that meaning. \textit{Transparency} is another mode of \textit{interpretability} that conveys meaning by making the inner workings of a model visible to a human \parencite{Arieta2020}. Lastly, \textit{understandability} is a term often used as a synonym for \textit{interpretability} in software engineering \parencite{Alonso2018}.

The most important distinction for this thesis is between generative and discriminative models. Conceptually generative and discriminative \ac{AI} both use machine learning to learn patterns from data. They are however fundamentally different in their application. Discriminative models learn the decision boundary between two or more classes and are used for classifying data points into these existing classes. Common use cases include anomaly detection \parencite{Edozie2025, Hilal2022}, image classification \parencite{Lu2007}, and sentiment analysis \parencite{Wankhade2022}. Generative models on the other hand learn the underlying distribution of a dataset and can be used to generate new data points that are similar to the training data. Generative models are frequently used for text generation \parencite{Brown2020} or image synthesis \parencite{Rombach2021}.

In the context of \ac{XAI} generative and discriminative models differ in one key aspect: the type and size of output they produce. In an information theoretical sense, discriminative models produce a small amount of information, like a class label that can be expressed in possibly a few dozen bits in extreme cases \parencite{Schneider2024}. Furthermore, the output is often previously known in the form of existing classes. Generative models on the other hand produce a large amount of information, like a paragraph of text or an image, that can be expressed in the order of megabits \parencite{Schneider2024}. Furthermore, the output is often novel and not previously known. This difference in output has implications for explainability methods and how users interact with and perceive these models. Discriminative models can be designed to be inherently interpretable by using simple models like decision trees or linear regression \parencite{Rudin2019}. Alternatively post-hoc explainability methods can be used to explain complex models like deep neural networks \parencite{Ribeiro2016, Lundberg2017}. Generative models on the other hand are often complex and not interpretable by design, which limits the applicability of inherently interpretable models.

\subsection{Explainability Methods} \label{ssec:explainability_methods}

Much research in \ac{XAI} has focused on developing explainability methods for discriminative models. These methods can be broadly categorized into two groups: transparency-based methods and post-hoc explainability methods \parencite{Arieta2020}. Transparency-based methods aim to make the inner workings of a model visible to a human. This can be achieved by using simple models like decision trees or linear regression that are inherently interpretable \parencite{Rudin2019}. Post-hoc explainability methods on the other hand aim to explain the output of a model without making its inner workings visible. This can be achieved highlighting important features in the input data (like LIME \parencite{Ribeiro2016}) or by approximating the model with a simpler interpretable model (like SHAP \parencite{Lundberg2017}). While these methods have proven to be effective in explaining models, both approaches are very technical and are difficult to understand for non-experts \parencite{Martens2025}.

Recently, new explainability approaches have been proposed that seek to remedy these shortcomings. Counterfactual explanations \parencite{Wachter2017} provide explanations by explaining why an alternative output was not chosen. For example in a loan application, a counterfactual explanation could state that if the applicant had a higher income, the loan would have been approved. Another approach are narrative explanations which use natural language to construct a short narrative around SHAP or counterfactual explanations \parencite{Martens2025}.

A third approach involves \ac{CoT} explanations \parencite{Wei2022}, where \acp{LLM} generate intermediate reasoning steps that lead to the final output. These steps can enhance users' perception of transparency by allowing them to follow the model's reasoning process. Moreover, decomposing complex problems into smaller steps has been shown to improve the effectiveness of \acp{LLM} in problem-solving \parencite{Wei2022}. Notably, this technique can be applied to existing models without retraining, as demonstrated in this study. However, it is important to recognize that these intermediate steps are generated by the model itself and may be unfaithful or incorrect \parencite{Turpin2023, Schneider2024}. In a related line of research, \textcite{Lindsey2025} demonstrated that it is possible to identify and visualize specific neurons representing concepts, and to observe activation patterns that resemble a thought process.

\subsection{Dual Process Theory} \label{ssec:dual_process}

Dual process theory refers to a group of theories in cognitive psychology that proposes humans have two distinct types of cognitive processing: Type 1 and Type 2 \parencite{Evans2013}. Type 1 processing is intuitive, autonomous processing and often referred to as “System 1” \parencite{Kahnemann2011}. Different theories propose different additional characteristics of Type 1 processing (see Table \ref{tab:dual_process_characteristics}), but common characteristics focus on the autonomous and intuitive nature of Type 1 processing. Type 2 processing (also “System 2”) on the other hand is deliberate, controlled processing that requires cognitive effort \parencite{Evans2013}. Type 2 processing is often associated with higher-order cognitive functions like reasoning, problem-solving, and decision-making \parencite{Kahnemann2011}.

\begin{ctable}
    \begin{tabularx}{0.5\textwidth}{l|l}
        \textbf{Type 1} & \textbf{Type 2} \\
        \hline
        Autonomous & Deliberate \\
        Intuitive & Controlled \\
        \hline
        Fast & Slow \\
        Parallel & Serial \\
        Non-conscious & Conscious \\
        Biased responses & Normative responses \\
        Contextualized & Abstract \\
    \end{tabularx}
    \caption[Type 1 and Type 2 Characteristics]{Common characteristics of Type 1 and Type 2 processing in dual process theories \parencite{Evans2013}}
    \label{tab:dual_process_characteristics}
\end{ctable}

Different theories propose different relationships between Type 1 and Type 2 processing. Some theories propose a competitive relationship, where both types of processing work in parallel and resolve conflicts if they occur. However, this conflicts with the different systems are often associated with different speeds. If Type 1 processing is faster, conflict resolution would always have to wait for Type 2 processing to finish. Alternatively, a wide range of theories proposes a default-interventionist relationship, where Type 1 processing is the default mode of processing and Type 2 processing only intervenes when necessary \parencite{Evans2013}. There is evidence for both relationships \parencite{Evans2013}, even attempts to reconcile both relationships into a single theory \parencite{Djulbegovic2012}, but the exact relationship remains an open question. Recent research in the context of \ac{XAI} has interpreted results assuming the default-interventionist relationship is the more accurate one \parencite{Jussupow2021, Shin2021}.

\subsection{Heuristics} \label{ssec:heuristics}

Heuristics are methods for problem-solving that use practical and efficient approaches to produce solutions that are not guaranteed to be optimal, but can be achieved much quicker \parencite{Gigerenzer2011}. Heuristics are often contrasted with algorithms, which are systematic and exhaustive methods that guarantee an optimal solution. The research into heuristics goes back on the work of \cite{Simon1955}, who observed that humans often make decisions with limited time, information, and cognitive resources. This results in a behaviour he coined as “satisficing” \parencite{Simon1956} as a portmanteau of “satisfy” and “suffice”. It seeks to describe the behaviour of accepting the first satisfactory solution instead of searching for the optimal one.

Building on this work, \textcite{Tversky1974} identified three key heuristics that humans use in decision-making:

\begin{itemize}
    \item The \textbf{Availability} heuristic serves to estimate the likelihood of an event based on how easily examples come to mind. For example, if a person has recently heard about a plane crash, they might overestimate the risk of flying.
    \item \textbf{Representativeness} is used to judge the probability of an event based on how similar it is to a prototype. For example, if a person sees someone who is tall and athletic, they might assume that they are a basketball player.
    \item \textbf{Anchoring and Adjustment} describe a related set of behaviours where people rely heavily on an initial piece of information (the anchor) when making decisions. For example, if a person is negotiating a salary, they might start with a high anchor and then adjust it downwards.
\end{itemize}

Due to their nature of leaving out information and relying on cognitive shortcuts, heuristics can lead to systematic biases and erroneous decisions. They are also related to dual process theory, as they are often associated with Type 1 processing \parencite{Evans2013}.

\subsection{Metacognition} \label{ssec:metacognition}

\subsection{Cognitive Offloading} \label{ssec:cognitive_offloading}

% % New proposed structure for the section:
% \subsection{Explainable Artificial Intelligence (XAI)} \label{ssec:xai}

% \ac{XAI}

% \cite{Arieta2020}

% \subsubsection{Discriminative vs. Generative Models} \label{sssec:disc_vs_gen}

% \subsubsection{Transparency and Post-hoc Explainability} \label{sssec:transparency_posthoc}

% \subsubsection{Established Explainability Methods} \label{sssec:established_methods}

% \cite{Ribeiro2016}

% \cite{Lundberg2017}

% \subsubsection{Novel Explainability Approaches} \label{sssec:novel_approaches}

% \cite{Martens2025}

% \subsection{Cognitive Psychology} \label{ssec:cognitive_psychology}

% \subsubsection{Dual Process Theory and Heuristics} \label{sssec:dual_process}

% \cite{Kahnemann2011}

% \subsubsection{Cognitive Offloading} \label{sssec:cognitive_offloading}

% \subsubsection{Metacognition} \label{sssec:metacognition}

% \subsubsection{Psychology of Explanations} \label{sssec:psychology_explanations}

% \cite{Miller2019}

% \subsubsection{Cognition and Metacognition of AI Decisions} \label{sssec:cognition_metacognition_ai}

% \cite{Jussupow2021}

% \cite{Shin2021}
