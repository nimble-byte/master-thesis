\section{State of Research} \label{sec:state_of_research}

The following section provides an overview of the current state of research in \ac{XAI} and cognitive psychology. It defines key terminology, presents notable explainability methods, and discusses relevant psychological theories and findings.
% TODO: note strucutre

\subsection{Terminology} \label{ssec:terminology}

% use distinction of "interpretability" vs. "explainibility" in cognitive psychology as hook to define termns

While in the field of \ac{XAI} the terms \textit{interpretability}, \textit{explainability} and \textit{understandability} are often used interchangeably, in cognitive psychology they have distinct meanings. \textit{Interpretability} is a broad concept that encompasses various ways to “provide the meaning in understandable terms to a human” \parencite{Arieta2020}. \textit{Explainability} is one mode of these modes \parencite{Lipton2016}, that uses an explanation to convey that meaning. \textit{Transparency} is another mode of \textit{interpretability} that conveys meaning by making the inner workings of a model visible to a human \parencite{Arieta2020}. Lastly, \textit{understandability} is a term often used as a synonym for \textit{interpretability} in software engineering \parencite{Alonso2018}.

The most important distinction for this thesis is between generative and discriminative models. Conceptually generative and discriminative \ac{AI} both use machine learning to learn patterns from data. They are however fundamentally different in their application. Discriminative models learn the decision boundary between two or more classes and are used for classifying data points into these existing classes. Common use cases include anomaly detection \parencite{Edozie2025, Hilal2022}, image classification \parencite{Lu2007}, and sentiment analysis \parencite{Wankhade2022}. Generative models on the other hand learn the underlying distribution of a dataset and can be used to generate new data points that are similar to the training data. Generative models are frequently used for text generation \parencite{Brown2020} or image synthesis \parencite{Rombach2021}.

In the context of \ac{XAI} generative and discriminative models differ in one key aspect: the type and size of output they produce. In an information theoretical sense, discriminative models produce a small amount of information, like a class label that can be expressed in possibly a few dozen bits in extreme cases \parencite{Schneider2024}. Furthermore, the output is often previously known in the form of existing classes. Generative models on the other hand produce a large amount of information, like a paragraph of text or an image, that can be expressed in the order of megabits \parencite{Schneider2024}. Furthermore, the output is often novel and not previously known. This difference in output has implications for explainability methods and how users interact with and perceive these models. Discriminative models can be designed to be inherently interpretable by using simple models like decision trees or linear regression \parencite{Rudin2019}. Alternatively post-hoc explainability methods can be used to explain complex models like deep neural networks \parencite{Ribeiro2016, Lundberg2017}. Generative models on the other hand are often complex and not interpretable by design, which limits the applicability of inherently interpretable models.

% % New proposed structure for the section:
% \subsection{Explainable Artificial Intelligence (XAI)} \label{ssec:xai}

% \ac{XAI}

% \cite{Arieta2020}

% \subsubsection{Discriminative vs. Generative Models} \label{sssec:disc_vs_gen}

% \subsubsection{Transparency and Post-hoc Explainability} \label{sssec:transparency_posthoc}

% \subsubsection{Established Explainability Methods} \label{sssec:established_methods}

% \cite{Ribeiro2016}

% \cite{Lundberg2017}

% \subsubsection{Novel Explainability Approaches} \label{sssec:novel_approaches}

% \cite{Martens2025}

% \subsection{Cognitive Psychology} \label{ssec:cognitive_psychology}

% \subsubsection{Dual Process Theory and Heuristics} \label{sssec:dual_process}

% \cite{Kahnemann2011}

% \subsubsection{Cognitive Offloading} \label{sssec:cognitive_offloading}

% \subsubsection{Metacognition} \label{sssec:metacognition}

% \subsubsection{Psychology of Explanations} \label{sssec:psychology_explanations}

% \cite{Miller2019}

% \subsubsection{Cognition and Metacognition of AI Decisions} \label{sssec:cognition_metacognition_ai}

% \cite{Jussupow2021}

% \cite{Shin2021}
