\section{Discussion} \label{sec:discussion}

The discussion section of this thesis will be split into three main parts: A summary of the main findings and the implications for the hypotheses and research questions posed in the introduction are presented in Section~\ref{ssec:summary}. Section \ref{ssec:implications} discusses the implications of these findings and suggests avenues for future research building on this work. Finally, Section \ref{ssec:limitations} addresses the limitations of the current study and how they might be mitigated in future work.

\subsection{Summary of Findings} \label{ssec:summary}

This thesis investigates the cognitions and metacognitions of \ac{LLM} users in problem-solving tasks and the effect explanations have on these processes. Current research primarily focuses on methods for generating explanations, that are understandable to lay users, and their effects on task performance. This study sought to investigate how the underlying cognitive and metacognitive processes are affected by explanations.

Firstly, the study finds that problem solvers using \acp{LLM} exhibit a range of collaboration strategies, where the role of the \ac{LLM} can vary from an interactive knowledge base to a collaborative partner. The choice of strategy is influenced by task-dependent factors, such as the complexity of the problem, the user's perception of their own expertise, and the perceived capabilities of the system. The decision for a given strategy is subconscious, likely based on heuristic assessments of the aforementioned factors. Users can also change their strategy dynamically based on their evolving perceptions of the task and the model's performance. This suggests an ongoing evaluation of the interaction with the \ac{LLM}. Explanations have no effect on the choice of collaboration strategy.

Secondly, users employ a variety of cognitive and metacognitive processes when encountering conflicts when interacting with \acp{LLM}. Processes like \textit{self-monitoring} and \textit{system monitoring} are used to manage these conflicts. The choice and effectiveness of these processes are influenced by similar factors as the collaboration strategies, including task complexity, user expertise, and system capabilities. However, the availability of explanation has no effect on these processes. In addition, users also exhibit learning effects, where they adapt their problem-solving strategies based on their experiences with the \ac{LLM} over time. This includes adjusting their expectations of the \ac{LLM}'s capabilities and modifying their interaction strategies to better leverage the \ac{LLM}'s strengths and mitigate its weaknesses.

Thirdly, providing explanations for the systems' outputs has effects on users' perception of the \ac{LLM}, as well as their perception of the collaboration. Explanations have desirable effects on users' acceptance, self-efficacy, and perception of their own effort and performance, with statistically significant effects on \textit{perceived usefulness} and \textit{frustration}.

\subsection{Implications \& Future Work} \label{ssec:implications}

The findings in this thesis have several implications for the design of systems based on generative \ac{AI} and for future research in this area. Firstly, the findings suggest, that the cognitive and metacognitive patterns observed in users of discriminative \ac{AI} systems are also present when using \acp{LLM}. This suggests that existing theories and models of human-AI interaction can be applied to \acp{LLM}, but may need to be adapted to account for the unique characteristics of \acp{LLM}, such as their generative capabilities and the nature of their errors.

Secondly, results indicate that additional cognitive and metacognitive processes are involved in determining if and how to utilize a \ac{LLM} during the problem-solving process. These processes are based on heuristic evaluations of a number of task- and subject-specific factors, and can change throughout the problem-solving process. The reflection and learning effects observed in the study suggest that users adapt their strategies based on their experiences with the system, indicating a dynamic interaction process. Future research could investigate these processes in more detail, exploring how users' perceptions of the \ac{LLM} evolve over time and how this affects their interaction strategies. This could be accomplished through investigation with a shorter questionnaire after each task to capture changes in perception over time within subjects. Additionally, this has implications for the design of \ac{LLM}-based systems, suggesting that systems should be designed to support users in these dynamic interactions.

Thirdly, the role of explanations on cognition and metacognition remains unclear. The tasks used in this study proved too simple to generate sufficiently deep collaboration that requires explanations by the system. Future work could investigate the role of explanations in more complex tasks that require deeper collaboration between the user and the \ac{LLM}. This could involve tasks that require multiple steps, complex reasoning, or creative problem-solving. Additionally, future research could explore different types of explanations, such as contrastive explanations or counterfactual explanations, to determine their effects on users' cognitive and metacognitive processes. This could be accomplished by increasing task complexity, as demonstrated by \cite{Kazemitabaar2024}, who investigated the use of interactive task decomposition to improve steering and verification in AI-assisted data analysis.

\subsection{Limitations} \label{ssec:limitations}

Based on the methodology and the design on the study this work has several limitations, that need to be considered when interpreting the findings. Limitations were mitigated wherever possible during the design and execution of the study, but some limitations remain.

Firstly, the study was designed, executed, and analysed by a single researcher. This introduces potential biases in all phases of the research process, from the design of the study to the interpretation of the results. This applies to the selection of tasks, the selection of models (including system prompts), the coding of qualitative data, and the interpretation of findings. To mitigate these limitations, established frameworks and methods were used wherever possible, e.g. using standardized questionnaires.

Secondly, the sample size of 12 participants in the final analysis is relatively small. While a sample size of 12 allows identifying themes and gather indications for patterns, it limits the generalizability of the findings. The small sample size also limits the statistical power of the quantitative analysis. To mitigate the limitation statistical tests suitable for small sample sizes were used, e.g. non-parametric tests like the Mann-Whitney U test. Despite the mitigation measures, the findings should be interpreted as indications and validated in future work with larger sample sizes.

Thirdly, the think-aloud methodology used in the study has inherent limitations. \cite{VanSomeren1994} note that participant may be afraid of being judged based on their verbalizations, which may lead to participants censoring their thoughts. This effect may be amplified when participants have a pre-existing relationship with the researcher, as was the case in this study. To mitigate this limitation, participants were assured that there were no right or wrong answers and that their honest thoughts and feelings were valued. Additionally, a cover story was used that framed the study as an investigation of \acp{LLM} rather than the participants' problem-solving abilities. However, despite these mitigation measures, the potential for social desirability bias remains \cite{Krumpal2013}.

Lastly, the study used a number of standardized questionnaires, which were translated without formal validation. While care was taken to ensure the translations were accurate and preserved the meaning of the original items, the lack of formal validation means that the psychometric properties of the translated questionnaires are unknown. This limitation may affect the reliability and validity of the quantitative findings. Additionally, the questionnaire was only answered once by participants, after all tasks had been completed, to counteract the length of 28 total questions. However, this eliminates the possibility to perform within-subject comparisons to detect changes in perception. This limitation could be mitigated in future work by developing a shorter questionnaire, that can be answered after each task.
