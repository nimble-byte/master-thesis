\section*{Abstract}

The rapid advancement of generative Artificial Intelligence (AI) has transformed various sectors, introducing sophisticated models capable of producing human-like text, images, and other media. Research in Explainable AI (XAI) has lagged behind these advancements. The cognition and metacognition of users interacting with AI, have also been underexplored, particularly in the context of generative AI. This thesis investigates the effects of explanations for generative AI on users' cognition and metacognition. A user study was conducted to evaluate how explanations for generative AI influence the problem-solving strategies, cognitive and metacognitive processes of users and their perceptions of the AI system. The study involved participants solving maths problems with the assistance of a chatbot. The chatbot either provided Chain-of-Thought (CoT) explanations or no explanations. The results of this formative study with 12 participants indicate, that users utilize metacognitive planning, monitoring, and evaluation processes to steer their interaction with the AI. Challenges arise when users disagree with the AI's suggestions, requiring the user to switch to more systematic processing strategies to resolve the conflict. Furthermore, the presence of explanations was found to significantly improve the perceived usefulness of the model and reduce frustration of users. Additionally, the findings indicate positive effects on users' self-efficacy, mental load and the perceived ease of use, although these effects were not statistically significant. These findings contribute to the understanding of human-AI interaction in the context of generative AI and highlight the importance of explainability for enhancing user experience and trust in AI systems. The insights gained from this study build a foundation for future research in the cognition and metacognition of users interacting with generative AI, guiding the development of more effective and user-friendly AI systems.
